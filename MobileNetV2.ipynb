{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFu1eKbi8uj6"
      },
      "source": [
        "import pdb "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA2n7FT4r6qD"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS-AQt5or9hF"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeoS5EeiD66h",
        "outputId": "0c0e3292-2524-4f16-fefd-69a3fdc77e3d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8gyJNpnBsaK"
      },
      "source": [
        "### Unzip file into directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34PkGBUEHdJ"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/gdrive/MyDrive/Aesthetic/style_image_test.zip\",\"r\") as zip_ref: \n",
        "  zip_ref.extractall(\"/content/data/\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RmCDZcQsB3P"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/gdrive/MyDrive/Aesthetic/style_image_train.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"/content/data/\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3tLgE4EBw4Y"
      },
      "source": [
        "### Define our dataset class and transforms. \n",
        "Note these transforms are just the sample taken from the pytorch VGG documentation\n",
        "\n",
        "Dataset class code adapted from https://pytorch.org/tutorials/recipes/recipes/custom_dataset_transforms_loader.html\n",
        "\n",
        "dataset returns an object with image and attribute keys. Attribute is a numpy array of attributes as described in the readme:\n",
        "\n",
        "Columns 0 - 9: Counts of aesthetics ratings on a scale of 1-10. Column 0 \n",
        "has counts of ratings of 1 and column 9 has counts of ratings of 10.\n",
        "\n",
        "Columns 10 - 11: Semantic tag IDs. There are 66 IDs ranging from 1 to 66.\n",
        "The file tags.txt contains the textual tag corresponding to the numerical\n",
        "id. Each image has between 0 and 2 tags. Images with less than 2 tags have\n",
        "a \"0\" in place of the missing tag(s).\n",
        "\n",
        "Column 12: Challenge ID. The file challenges.txt contains the name of \n",
        "the challenge corresponding to each ID.\n",
        "\n",
        "Column 13-26: Style ID one hot encoded (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG7eJz73ipJ-"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    \n",
        "    transforms.CenterCrop(227),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def pil_loader(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    with Image.open(f) as img:\n",
        "      return img.convert('RGB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIYuFN1LSoMp"
      },
      "source": [
        "class AVADataset1(Dataset):\n",
        "    \"\"\"AVA dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, image_root_dir, ava_filepath, image_id_filepath, image_style_filepath=None, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ava_file (string): Path to the csv file with annotations.\n",
        "            aesthetic_file (string): Path to the csv file with image ids\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = image_root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read in data into pandas data frames\n",
        "        self.ava_frame = pd.read_csv(\n",
        "            ava_filepath, \n",
        "            sep=' ', \n",
        "            header=None, \n",
        "            names=[\"idx_old\", \"image_id\", \"1_count\", \"2_count\", \"3_count\", \"4_count\", \"5_count\", \"6_count\", \"7_count\", \"8_count\", \"9_count\", \"10_count\", \"semantic_1_id\", \"semantic_2_id\", \"challenge_id\"]\n",
        "            )\n",
        "\n",
        "        imageid_frame = pd.read_csv(image_id_filepath, sep=' ', header=None, names=['id'])\n",
        "\n",
        "        # remove examples without an attribute tag (where tag == 0)\n",
        "        self.ava_frame.drop(self.ava_frame[self.ava_frame.semantic_1_id == 0].index, inplace=True)\n",
        "\n",
        "        # one hot encode the first semantic tag and drop tag columns\n",
        "        lb = preprocessing.LabelBinarizer()\n",
        "        lb.fit(list(range(1,67)))\n",
        "        self.ava_frame = pd.concat([self.ava_frame, pd.DataFrame(lb.transform(self.ava_frame.semantic_1_id), columns=[\"semantic_id_\" + str(x) for x in range(1,67)])], axis=1)\n",
        "        self.ava_frame.drop(columns=['semantic_1_id','semantic_2_id'], inplace=True)\n",
        "\n",
        "        # if a style image file is given, add that data as well\n",
        "        if image_style_filepath != None:\n",
        "          image_style_frame = pd.read_csv(image_style_filepath, sep=' ', header=None)\n",
        "          # if the style only has one value we need to one hot encode it\n",
        "          if len(image_style_frame.columns) == 1:\n",
        "            image_style_frame = pd.get_dummies(image_style_frame[0], prefix='style_')\n",
        "          imageid_frame = image_style_frame.join(imageid_frame, rsuffix='_id', lsuffix='_style')\n",
        "\n",
        "        # merge frames\n",
        "        self.ava_frame = self.ava_frame.merge(imageid_frame, left_on=\"image_id\", right_on='id', how='inner')\n",
        "\n",
        "        # remove last column which is duplicate id column\n",
        "        self.ava_frame.pop(self.ava_frame.columns[-1])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ava_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        img_id = int(self.ava_frame.iloc[idx, 1])\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                f\"{img_id}.jpg\")\n",
        "\n",
        "        # image = Image.fromarray(io.imread(img_name))\n",
        "        image = pil_loader(img_name)\n",
        "        attributes = np.array(self.ava_frame.iloc[idx, 2:]).astype('float')\n",
        "\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        label = np.array(self.ava_frame.iloc[idx,2:12].astype('float'))\n",
        "        label = label / label.sum()\n",
        "        \n",
        "\n",
        "        sample = {\"id\": img_id, 'image': image, 'attributes': attributes,'label':label}\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-U4TEG_71ZE"
      },
      "source": [
        "dataset_train = AVADataset1(ava_filepath=\"/content/gdrive/MyDrive/Aesthetic/style_image_train/AVA.txt\", \n",
        "                     image_id_filepath=\"/content/gdrive/MyDrive/Aesthetic/style_image_train/train.jpgl\",\n",
        "                     image_root_dir=\"/content/gdrive/MyDrive/Aesthetic/style_image_train/images\", \n",
        "                     #target_path=\"/content/gdrive/MyDrive/Aesthetic/target_train.csv\",\n",
        "                     transform=preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82UoPTG3DPsa"
      },
      "source": [
        "dataset_test = AVADataset1(ava_filepath=\"/content/gdrive/MyDrive/Aesthetic/style_image_train/AVA.txt\", \n",
        "                     image_id_filepath=\"/content/gdrive/MyDrive/Aesthetic/style_image_test/test.jpgl\", \n",
        "                     image_root_dir=\"/content/gdrive/MyDrive/Aesthetic/style_image_test\",\n",
        "                     #target_path=\"/content/gdrive/MyDrive/Aesthetic/target_test.csv\", \n",
        "                     transform=preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL4enHeO-nFq",
        "outputId": "93733a6f-3247-48c1-b72f-74801ee9b3db"
      },
      "source": [
        "r=dataset_train[0]['image']\n",
        "r.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X2osntiaIof"
      },
      "source": [
        "### Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqwN_fquSZ-5"
      },
      "source": [
        "## MobileNet implementation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-zJxJUXcarb",
        "outputId": "84be0e00-3d84-4f3e-d11e-fc6f59a6c905"
      },
      "source": [
        "!pip install common"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting common\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/b2/c900168d36abd28b1b08a81387835eff8b574bc6c2e9fefb5c4a38135d94/common-0.1.2.tar.gz\n",
            "Building wheels for collected packages: common\n",
            "  Building wheel for common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for common: filename=common-0.1.2-cp37-none-any.whl size=3734 sha256=19c46678e2e5d6d7b863c329935a57a498ae6a1c8b7f3af4bc38dc9ae48aecdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/8f/ec/9ac55fd8f7923ddf23619c89b42dbbcfc71db6ee41ad5e7b5e\n",
            "Successfully built common\n",
            "Installing collected packages: common\n",
            "Successfully installed common-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbphnD44arEG"
      },
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            # pw\n",
        "            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(inp * expand_ratio),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            # dw\n",
        "            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, 3, stride, 1, groups=inp * expand_ratio, bias=False),\n",
        "            nn.BatchNorm2d(inp * expand_ratio),\n",
        "            nn.ReLU6(inplace=True),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(oup),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        # setting of inverted residual blocks\n",
        "        self.interverted_residual_setting = [\n",
        "            # t, c, n, s\n",
        "            [1, 16, 1, 1],\n",
        "            [6, 24, 2, 2],\n",
        "            [6, 32, 3, 2],\n",
        "            [6, 64, 4, 2],\n",
        "            [6, 96, 3, 1],\n",
        "            [6, 160, 3, 2],\n",
        "            [6, 320, 1, 1],\n",
        "        ]\n",
        "\n",
        "        # building first layer\n",
        "        assert input_size % 32 == 0\n",
        "        input_channel = int(32 * width_mult)\n",
        "        self.last_channel = int(1280 * width_mult) if width_mult > 1.0 else 1280\n",
        "        self.features = [conv_bn(3, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in self.interverted_residual_setting:\n",
        "            output_channel = int(c * width_mult)\n",
        "            for i in range(n):\n",
        "                if i == 0:\n",
        "                    self.features.append(InvertedResidual(input_channel, output_channel, s, t))\n",
        "                else:\n",
        "                    self.features.append(InvertedResidual(input_channel, output_channel, 1, t))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
        "        self.features.append(nn.AvgPool2d(input_size // 32))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(self.last_channel, n_class),\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.last_channel)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                n = m.weight.size(1)\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def mobile_net_v2(pretrained=True):\n",
        "    model = MobileNetV2()\n",
        "    if pretrained:\n",
        "        path_to_model = '/content/gdrive/MyDrive/Aesthetic/mobilenetv2.pth.tar'\n",
        "\n",
        "        state_dict = torch.load(path_to_model, map_location=lambda storage, loc: storage)\n",
        "        #for key in [\"features.18.0.weight\", \"features.18.1.weight\", \"features.18.1.bias\", \"features.18.1.running_mean\", \"features.18.1.running_var\"]:\n",
        "          #del state_dict[key]\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9o6vY-Kr__6"
      },
      "source": [
        "NIMA model from based on https://arxiv.org/abs/1709.05424"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvtza6exbO4l"
      },
      "source": [
        "class NIMA(nn.Module):\n",
        "    def __init__(self, pretrained_base_model=True):\n",
        "        super(NIMA, self).__init__()\n",
        "        base_model = mobile_net_v2(pretrained=pretrained_base_model)\n",
        "        base_model = nn.Sequential(*list(base_model.children())[:-1])\n",
        "\n",
        "        #base_model = nn.Sequential(*list(base_model.modules())[:-1])\n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.base_model = base_model\n",
        "\n",
        "        self.Hl = nn.Sequential(\n",
        "            #nn.Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), \n",
        "            #nn.BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            #nn.ReLU(inplace=True),\n",
        "            #nn.AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
        "            #nn.Flatten(),\n",
        "            nn.ReLU(False),\n",
        "            nn.Dropout(p=0.75),\n",
        "            nn.Linear(1280, 10),\n",
        "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "        self.att = nn.Sequential(\n",
        "            #nn.Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), \n",
        "            #nn.BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            #nn.ReLU(inplace=True),\n",
        "            #nn.AvgPool2d(kernel_size=7, stride=7, padding=0),\n",
        "            #nn.Flatten(),\n",
        "            nn.ReLU(False),\n",
        "            nn.Dropout(p=0.75),\n",
        "            nn.Linear(1280, 66),\n",
        "            #nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.ReLU(False),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        #print(x.size())\n",
        "        #x=self.avgpool(x)\n",
        "        #print(x.size())\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #print(x.size())\n",
        "        yhl= self.Hl(x)\n",
        "        yatt=self.att(x)\n",
        "        #yatt=F.sigmoid(yatt)\n",
        "        return yhl,yatt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQGHsphwPo__"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgtI98rZ4Odi"
      },
      "source": [
        "def train(train_loader, model, device, criterion_hl, criterion_attr, optimizer, epoch):\n",
        "  model.train()\n",
        "  scores_arr = np.arange(1,11) #array for calculating average scores\n",
        "  tscores=[]\n",
        "  for i_batch, batch in enumerate(train_loader):\n",
        "    # calculate average score for each image\n",
        "    #print (\"batch index {}, 0/1: {}/{}\".format(i_batch,len(np.where(batch[\"target\"].numpy() == 0)[0]),len(np.where(batch[\"target\"].numpy() == 1)[0])))\n",
        "    \n",
        "\n",
        "    # from average image score calculate binary label high or low\n",
        "    score_target = batch[\"label\"]\n",
        "\n",
        "    # get semantic tag target\n",
        "    attr_target = np.argmax(batch[\"attributes\"][:,13:], axis=1)\n",
        "\n",
        "    # # get style target\n",
        "    # attr_target = batch[\"attributes\"][:,13:]\n",
        "    \n",
        "    output = model(batch[\"image\"].to(device))\n",
        "    #print(output[0])\n",
        "    score_target=torch.tensor(score_target,device=device)\n",
        "    #pred_prob_hl=output[0].cpu().detach().numpy()\n",
        "    \n",
        "    #pred_scores_hl=np.argmax(pred_prob_hl,axis=1)\n",
        "    #pred_scores_hl=np.where(pred_prob_hl > 5, 1, 0)\n",
        "  \n",
        "    \n",
        "      \n",
        "    #pred = torch.round(outputr)\n",
        "\n",
        "\n",
        "    # calculate loss (non-weighted)\n",
        "    loss_hl = criterion_hl(output[0],score_target)\n",
        "    \n",
        "    # calculate hl loss weighted\n",
        "    # loss_hl = criterion_hl(output[0], torch.tensor(score_target, device=device), reduction='none')\n",
        "    # loss_weights = np.where((avg_scores > 6) | (avg_scores < 4), 6, 1)\n",
        "    # loss_hl = loss_hl * loss_weights\n",
        "    # loss_hl = torcn.mean(loss_hl)\n",
        "    #pred_prob_att=output[1].cpu().detach().numpy()\n",
        "    \n",
        "    #pred_scores_att=np.argmax(pred_prob_att,axis=1)\n",
        "    loss_attr = criterion_attr(output[1], torch.tensor(attr_target, device=device))\n",
        "    loss = loss_hl + loss_attr\n",
        "    #loss = loss_hl\n",
        "    if i_batch % 50 == 0:\n",
        "      print(f\"loss: {loss},losshl:{loss_hl}\")\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # if i_batch+1 % 100 == 0:\n",
        "    #   break\n",
        "\n",
        "def test(test_loader, model, device):\n",
        "  model.eval()\n",
        "  scores_arr = np.arange(1,11) #array for calculating average scores\n",
        "  total_loss_style = 0.\n",
        "  correct = 0.\n",
        "  tscores=[]\n",
        "  ret=[]\n",
        "  with torch.no_grad():\n",
        "    for i_batch, batch in enumerate(test_loader):\n",
        "      avg_scores = (batch[\"attributes\"][:,:10] * scores_arr).sum(axis=1) / batch[\"attributes\"][:,:10].sum(axis=1)\n",
        "\n",
        "      # from average image score calculate binary label high or low\n",
        "      score_target = torch.tensor(np.where(avg_scores > 5, 1, 0), device=device)\n",
        "      # calculate average score for each image\n",
        "     \n",
        "      #print(score_target)\n",
        "      # from average image score calculate binary label high or low\n",
        "    \n",
        "  \n",
        "      output = model(batch[\"image\"].to(device))\n",
        "     \n",
        "          \n",
        "      \n",
        "      prob=output[0].cpu().detach().numpy()\n",
        "      #print(\"prob=\",prob)\n",
        "      scores=prob*scores_arr\n",
        "      #print(\"scores=\",scores)\n",
        "      avg_s=np.sum(scores,axis=1)\n",
        "      #print(\"avg=\",avg_s)\n",
        "      score_pred = torch.tensor(np.where(avg_s > 5, 1, 0), device=device)\n",
        "      tscores.append(torch.argmax(output[1],dim=1))\n",
        "\n",
        "      #print(outputr,t)\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      correct = correct + score_pred.eq(score_target.view_as(score_pred)).sum().item()\n",
        "\n",
        "\n",
        "  print(f\"avg acc: {correct}/{len(test_loader.dataset)}\")\n",
        "  return tscores,ret\n",
        "  \n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\n",
        "      0.01 is the initial learning rate\n",
        "    \"\"\"\n",
        "    lr = 0.01 * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbku_spXQ2X"
      },
      "source": [
        "score_target=pd.read_csv(\"/content/gdrive/MyDrive/Aesthetic/target_train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni_3gvxPXJSi"
      },
      "source": [
        "class_sample_count=[]\n",
        "class_sample_count.append(score_target['0'].value_counts()[0])\n",
        "class_sample_count.append(score_target['0'].value_counts()[1])\n",
        "weight = [1. / csc for csc in class_sample_count]\n",
        "samples_weight = np.array([weight[t] for t in score_target['0']])\n",
        "\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "samples_weigth = samples_weight.double()\n",
        "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYjLPJL-NpNq"
      },
      "source": [
        "score_target=pd.read_csv(\"/content/gdrive/MyDrive/Aesthetic/target_scores.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lalvlX0jNoqt"
      },
      "source": [
        "weight = [1,7]\n",
        "samples_weight =[]\n",
        "\n",
        "for score in score_target['avg_score']:\n",
        "  if 4.5<score<6.5:\n",
        "    samples_weight.append(1/7.0)\n",
        "  else:\n",
        "    samples_weight.append(1.0/1.0)  \n",
        "\n",
        "samples_weight = torch.FloatTensor(samples_weight)\n",
        "samples_weigth = samples_weight.double()\n",
        "sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt0bjTIQaAC1"
      },
      "source": [
        "class EMDLoss(nn.Module):\n",
        "    \"\"\"Earch Mover's Distance(EMD) Loss in *Neural Image Assessment*.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, r=2, reduction='mean'):\n",
        "        super(EMDLoss, self).__init__()\n",
        "        self.r = r\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        cdf_pred = torch.cumsum(pred, -1)\n",
        "        cdf_target = torch.cumsum(target, -1)\n",
        "\n",
        "        samplewise_emd = (\n",
        "            torch.mean(torch.abs(cdf_pred - cdf_target) ** self.r, dim=-1) ** (1 / self.r)\n",
        "        )\n",
        "        if self.reduction is None:\n",
        "            return samplewise_emd\n",
        "        elif self.reduction == 'mean':\n",
        "            return torch.mean(samplewise_emd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFVfwKHQZoTE"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb8gPCauYtAj"
      },
      "source": [
        "no_cuda = False\n",
        "\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "train_loader = DataLoader(dataset_train,batch_size=16,sampler=sampler,\n",
        "                         num_workers=4)\n",
        "test_loader = DataLoader(dataset_test, batch_size=128,\n",
        "                        shuffle=False, num_workers=4)\n",
        "model = NIMA().to(device)\n",
        "\n",
        "criterion_attr = nn.CrossEntropyLoss()\n",
        "#criterion_hl = nn.CrossEntropyLoss()\n",
        "criterion_hl = EMDLoss()\n",
        "learning_rate = 0.01\n",
        "parameters = [\n",
        "   {\"params\": model.base_model.parameters()},\n",
        "   {\"params\": model.Hl.parameters(), \"lr\": 3e-5},\n",
        "   {\"params\": model.att.parameters(), \"lr\": 3e-5},\n",
        "]\n",
        "optimizer = torch.optim.Adam(parameters, lr=3e-5)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#optimizer = torch.optim.SGD(parameters, 0.0001, momentum=0.1, weight_decay=0.0001)\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(50):\n",
        "  print(epoch)\n",
        "  train(train_loader, model, device, criterion_hl, criterion_attr, optimizer, epoch)\n",
        "  if (epoch+1)%5==0:\n",
        "    model_name=\"/content/gdrive/MyDrive/Aesthetic/modelsampleattnew\"+str(epoch)+\".pt\"\n",
        "    torch.save(model.state_dict(), model_name)\n",
        "\n",
        "  tscores=test(test_loader, model, device)\n",
        "  #test(test_loader, model, device)\n",
        "\n",
        "  # adjust_learning_rate(optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILcB8Ky9pR0O"
      },
      "source": [
        "with open('/content/gdrive/MyDrive/Aesthetic/attSGD.txt', 'w') as f:\n",
        "    for item in tscores:\n",
        "        f.write(\"%s\\n\" % item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w-Ad9YpqGfR"
      },
      "source": [
        "import pickle\n",
        " \n",
        "with open('/content/gdrive/MyDrive/Aesthetic/attSGD.pkl', 'wb') as f:\n",
        "   pickle.dump(tscores, f)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPj6sLqeqK23"
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/gdrive/MyDrive/Aesthetic/model.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNaeDjQjFVI9"
      },
      "source": [
        "no_cuda = False\n",
        "test_loader = DataLoader(dataset_test, batch_size=128,\n",
        "                        shuffle=True, num_workers=4)\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "model = NIMA().to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Aesthetic/modelsampleattnew29.pt'))\n",
        "tscores,ret=test(test_loader, model, device)\n",
        "#output = model((dataset_test[420]['image']).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq49EdQ3sphl"
      },
      "source": [
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "model = NIMA().to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Aesthetic/modelsampleattnew39.pt'))\n",
        "#tscores,ret=test(test_loader, model, device)\n",
        "img = load_image(\"/content/gdrive/MyDrive/Aesthetic/style_image_test/820923.jpg\")\n",
        "input = preprocess_image(img)\n",
        "\n",
        "output = model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26liLs2Usgut"
      },
      "source": [
        "Bounding box calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Jk-VoS6AoD"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "import cv2\n",
        "try:\n",
        "    import google.colab\n",
        "    from google.colab.patches import cv2_imshow\n",
        "except:\n",
        "    from cv2 import imshow as cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class BBoxerwGradCAM():\n",
        "    \n",
        "    def __init__(self,learner,heatmap,image_path,resize_scale_list,bbox_scale_list,dir_path,i):\n",
        "        self.learner = learner\n",
        "        self.heatmap = heatmap\n",
        "        self.image_path = image_path\n",
        "        self.resize_list = resize_scale_list\n",
        "        self.scale_list = bbox_scale_list\n",
        "        self.dir_path=dir_path\n",
        "        self.i=i\n",
        "        \n",
        "        self.og_img, self.smooth_heatmap = self.heatmap_smoothing()\n",
        "        \n",
        "        #self.bbox_coords, self.poly_coords, self.grey_img, self.contours = self.form_bboxes()\n",
        "        \n",
        "        self.bboxes=self.form_bboxes()\n",
        "        \n",
        "    def heatmap_smoothing(self):\n",
        "        og_img = cv2.imread(self.image_path)\n",
        "        heatmap = cv2.resize(self.heatmap, (self.resize_list[0],self.resize_list[1])) # Resizing\n",
        "        og_img = cv2.resize(og_img, (self.resize_list[0],self.resize_list[1])) # Resizing\n",
        "        '''\n",
        "        The minimum pixel value will be mapped to the minimum output value (alpha - 0)\n",
        "        The maximum pixel value will be mapped to the maximum output value (beta - 155)\n",
        "        Linear scaling is applied to everything in between.\n",
        "        These values were chosen with trial and error using COLORMAP_JET to deliver the best pixel saturation for forming contours.\n",
        "        '''\n",
        "        index = self.image_path.find('style_image_test')\n",
        "        index2 = self.image_path.find('.')\n",
        "    #print(\"img path=\", img_path[index+16:index2])\n",
        "        self.path=self.image_path[index+16:index2]\n",
        "        heatmapshow = cv2.normalize(heatmap, None, alpha=0, beta=155, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "        heatmapshow = cv2.applyColorMap(heatmapshow, cv2.COLORMAP_JET)\n",
        "        #mask = (heatmap - np.min(heatmap)) / np.max(heatmap)\n",
        "\n",
        "        #heatmapshow = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "\n",
        "        #heatmapshow = np.float32(heatmap) / 255\n",
        "        \n",
        "        return og_img, heatmapshow\n",
        "    \n",
        "    def show_smoothheatmap(self):\n",
        "        heatmap_path=self.dir_path+'/shmap'\n",
        "        if not (os.path.isdir(heatmap_path)):\n",
        "          os.makedirs(heatmap_path)\n",
        "\n",
        "        gradcam_path = heatmap_path+'/'+str(self.i)+\".png\"\n",
        "\n",
        "        cv2.imwrite(gradcam_path, self.smooth_heatmap)\n",
        "        #cv2_imshow(self.smooth_heatmap)\n",
        "        #cv2.waitKey(0)\n",
        "        #cv2.destroyAllWindows()\n",
        "        \n",
        "    def show_bboxrectangle(self):\n",
        "        for bbox_coords,_,_,_  in self.bboxes:\n",
        "          cv2.rectangle(self.og_img,\n",
        "                        (bbox_coords[0],bbox_coords[1]),\n",
        "                        (bbox_coords[0]+bbox_coords[2],bbox_coords[1]+bbox_coords[3]),\n",
        "                        (0,0,0),3)\n",
        "          \n",
        "        #cv2_imshow(self.og_img)\n",
        "\n",
        "        heatmap_path=self.dir_path+'/bbox'\n",
        "        if not (os.path.isdir(heatmap_path)):\n",
        "          os.makedirs(heatmap_path)\n",
        "\n",
        "        gradcam_path = heatmap_path+'/'+str(self.i)+\".png\"\n",
        "\n",
        "        cv2.imwrite(gradcam_path, self.og_img)\n",
        "        #cv2.waitKey(0)\n",
        "        #cv2.destroyAllWindows()\n",
        "    \n",
        "    def show_contouredheatmap(self):\n",
        "        img_col = cv2.merge([self.grey_img,self.grey_img,self.grey_img]) # merge channels to create color image (3 channels)\n",
        "        cv2.fillPoly(img_col, self.contours, [36,255,12]) # fill contours on 3 channel image\n",
        "        cv2_imshow(img_col)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "        \n",
        "    def show_bboxpolygon(self):\n",
        "        for _,poly_coords,_,_  in self.bboxes:\n",
        "          cv2.polylines(self.og_img,poly_coords,True,(0,0,0),2)\n",
        "        heatmap_path=self.dir_path+'/poly'\n",
        "        if not (os.path.isdir(heatmap_path)):\n",
        "          os.makedirs(heatmap_path)\n",
        "\n",
        "        gradcam_path = heatmap_path+'/'+str(self.i)+\".png\"\n",
        "\n",
        "        cv2.imwrite(gradcam_path, self.og_img)\n",
        "        #cv2_imshow(self.og_img)\n",
        "        #cv2.waitKey(0)\n",
        "        #cv2.destroyAllWindows()\n",
        "    \n",
        "    def form_bboxes(self):\n",
        "      for thr in [127,80]:\n",
        "          grey_img = cv2.cvtColor(self.smooth_heatmap, cv2.COLOR_BGR2GRAY)\n",
        "          ret,thresh = cv2.threshold(grey_img,thr,255,cv2.THRESH_BINARY)\n",
        "          contours,hierarchy = cv2.findContours(thresh, 1, 2)\n",
        "          b=[]\n",
        "\n",
        "          for item in range(len(contours)):\n",
        "              cnt = contours[item]\n",
        "              #print(len(contours))\n",
        "              if len(cnt)>20:\n",
        "                  #print(len(cnt))\n",
        "                  x,y,w,h = cv2.boundingRect(cnt) # x, y is the top left corner, and w, h are the width and height respectively\n",
        "                  poly_coords = [cnt] # polygon coordinates are based on contours\n",
        "                  \n",
        "                  x = int(x*self.scale_list[0]) # rescaling the boundary box based on user input\n",
        "                  y = int(y*self.scale_list[1])\n",
        "                  w = int(w*self.scale_list[2])\n",
        "                  h = int(h*self.scale_list[3])\n",
        "\n",
        "                  #print(\"entered\")\n",
        "                  b.append([[x,y,w,h], poly_coords, grey_img, contours])\n",
        "              \n",
        "              else: \n",
        "                continue\n",
        "                #print(\"contour error (too small)\")\n",
        "          if b:\n",
        "              break\n",
        "\n",
        "      return b\n",
        "                \n",
        "    def get_bboxes(self):\n",
        "        return self.bbox_coords, self.poly_coords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-E_tMMGHRTs"
      },
      "source": [
        "image_resizing_scale = [227,227]\n",
        "bbox_scaling = [1,1,1,1] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6-p7dt58Um"
      },
      "source": [
        "\n",
        "\n",
        "bbox = BBoxerwGradCAM(model,\n",
        "                      gcam,\n",
        "                      \"/content/gdrive/MyDrive/Aesthetic/style_image_test/952233.jpg\",\n",
        "                      image_resizing_scale,\n",
        "                      bbox_scaling)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfn0ExUAyELn"
      },
      "source": [
        "## GradCam\n",
        "\n",
        "implementation taken from https://github.com/da2so/GradCAM_PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQvnMtYYyD_5"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/gdrive/MyDrive/Aesthetic/GradCAM_PyTorch.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"/\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK29q10hXuBu"
      },
      "source": [
        "\n",
        "class NoIndexError(Exception):\n",
        "    def __init__(self, message):\n",
        "        self.message = message\n",
        "\n",
        "class NoSuchNameError(Exception):\n",
        "    def __init__(self, message):\n",
        "        self.message = message"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAXCGcjZXRGu"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "\n",
        "\n",
        "#from exceptions import NoSuchNameError , NoIndexError\n",
        "\n",
        "def load_model(model_name):\n",
        "\n",
        "    #for saved model (.pt)\n",
        "    if '.pt' in model_name:\n",
        "        if torch.typename(torch.load(model_name)) == 'OrderedDict':\n",
        "\n",
        "            \"\"\"\n",
        "            if you want to use customized model that has a type 'OrderedDict',\n",
        "            you shoud load model object as follows:\n",
        "            \n",
        "            from Net import Net()\n",
        "            model=Net()\n",
        "            \"\"\"\n",
        "            model=NIMA()\n",
        "            model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "        else:\n",
        "            model = torch.load(model_name)\n",
        "\n",
        "    #for pretrained model (ImageNet)\n",
        "    elif hasattr(models , model_name):\n",
        "        model = getattr(models,model_name)(pretrained=True)\n",
        "    else:\n",
        "        print('Choose an available pre-trained model')\n",
        "        sys.exit()\n",
        "\n",
        "    model.eval()\n",
        "    if cuda_available():\n",
        "        model.cuda()\n",
        "\n",
        "    return model\n",
        "\n",
        "def cuda_available():\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    return use_cuda\n",
        "\n",
        "def load_image(path):\n",
        "    img = cv2.imread(path, 1)\n",
        "    img = cv2.resize(img, (227, 227))\n",
        "    img = np.float32(img) / 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img):\n",
        "    means = [0.485, 0.456, 0.406]\n",
        "    stds = [0.229, 0.224, 0.225]\n",
        "\n",
        "    preprocessed_img = img.copy()[:, :, ::-1]\n",
        "    for i in range(3):\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
        "        preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
        "    preprocessed_img = \\\n",
        "        np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
        "\n",
        "    if cuda_available():\n",
        "        preprocessed_img_tensor = torch.from_numpy(preprocessed_img).cuda()\n",
        "    else:\n",
        "        preprocessed_img_tensor = torch.from_numpy(preprocessed_img)\n",
        "\n",
        "    preprocessed_img_tensor.unsqueeze_(0)\n",
        "    return Variable(preprocessed_img_tensor, requires_grad=False)\n",
        "\n",
        "def save(mask, img, img_path,i):\n",
        "\n",
        "    mask = (mask - np.min(mask)) / np.max(mask)\n",
        "\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    gradcam = 1.0 * heatmap + img\n",
        "    gradcam = gradcam / np.max(gradcam)\n",
        "\n",
        "    #/content/gdrive/MyDrive/Aesthetic/style_image_test/149846.jpg\n",
        "    index = img_path.find('style_image_test')\n",
        "    index2 = img_path.find('.')\n",
        "    #print(\"img path=\", img_path[index+16:index2])\n",
        "    path=img_path[index+16:index2]\n",
        "    #print(path)\n",
        "    \n",
        "    dir_path=\"/content/gdrive/MyDrive/Aesthetic/content/GradCAM_PyTorch/resultnew/Attmap\"+path\n",
        "    #path = 'result/' + img_path[index + 1:index2] +'/'+model_path\n",
        "    if not (os.path.isdir(dir_path)):\n",
        "       os.makedirs(dir_path)\n",
        "\n",
        "    gradcam_path = dir_path+ '/'+str(i)+\".png\"\n",
        "\n",
        "    cv2.imwrite(gradcam_path, np.uint8(255 * gradcam))\n",
        "    return dir_path\n",
        "    \n",
        "def isInt_str(v):\n",
        "    v = str(v).strip()\n",
        "    return v == '0' or (v if v.find('..') > -1 else v.lstrip('-+').rstrip('0').rstrip('.')).isdigit()\n",
        "\n",
        "def choose_tlayer(model_obj):\n",
        "    name_to_num = {}\n",
        "    sel_module = False\n",
        "    name_module = None\n",
        "    module_list = ['Sequential','Bottleneck','container','Block','densenet']\n",
        "    while True:\n",
        "        for num, module in enumerate(model_obj.named_children()):\n",
        "            if any(x in torch.typename(module[1]) for x in module_list): \n",
        "                print(f'[ Number: {num},  Name: {module[0]} ] -> Module: {module[1]}\\n')\n",
        "                name_to_num[module[0]] = num\n",
        "            else:\n",
        "                print(f'[ Number: {num},  Name: {module[0]} ] -> Layer: {module[1]}\\n')\n",
        "                name_to_num[module[0]] = num\n",
        "\n",
        "        print('<<      You sholud not select [classifier module], [fc layer] !!      >>')\n",
        "        if sel_module == False:\n",
        "            a = input('Choose \"Number\" or \"Name\" of a module containing a target layer or a target layer: ')\n",
        "        else:\n",
        "            a = input(f'Choose \"Number\" or \"Name\" of a module containing a target layer or a target layer in {name_module} module: ')\n",
        "\n",
        "        print('\\n'*3)\n",
        "        m_val = list(model_obj._modules.values())\n",
        "        m_key = list(model_obj._modules.keys())\n",
        "        if isInt_str(a) == False:\n",
        "            a = name_to_num[a]\n",
        "        try:\n",
        "            if any(x in torch.typename(m_val[int(a)]) for x in module_list): \n",
        "                model_obj = m_val[int(a)]\n",
        "                name_module = m_key[int(a)]\n",
        "                sel_module = True\n",
        "            else:\n",
        "                t_layer = m_val[int(a)]\n",
        "                return t_layer\n",
        "\n",
        "        except IndexError:\n",
        "            raise NoIndexError('Selected index (number) is not allowed.')\n",
        "        except KeyError:\n",
        "            raise NoSuchNameError('Selected name is not allowed.')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNebt_06jsAs"
      },
      "source": [
        "import numpy as np\n",
        "import sys \n",
        "import cv2\n",
        "\n",
        "import torch \n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "class GradCAM():\n",
        "    def __init__(self, path, model_path, select_t_layer, class_index = None):\n",
        "\n",
        "        self.img_path = path\n",
        "        #self.model_path = model_path\n",
        "        self.class_index = class_index\n",
        "        self.select_t_layer = select_t_layer\n",
        "        \n",
        "        # Save outputs of forward and backward hooking\n",
        "        self.gradients = dict()\n",
        "        self.activations = dict()\n",
        "        \n",
        "        self.model = model_path\n",
        "        \n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients['value'] = grad_output[0]\n",
        "            return None\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations['value'] = output\n",
        "            return None\n",
        "        \n",
        "        #find finalconv layer name\n",
        "\n",
        "        if self.select_t_layer == False:\n",
        "            model_obj=self.model\n",
        "            name_to_num = {}\n",
        "            sel_module = -1\n",
        "            next=-1\n",
        "            name_module = None\n",
        "            module_list = ['Sequential','Bottleneck','container','Block','densenet']\n",
        "            run=True\n",
        "            while run:\n",
        "                for num, module in enumerate(model_obj.named_children()):\n",
        "                    if any(x in torch.typename(module[1]) for x in module_list): \n",
        "                        #print(f'[ Number: {num},  Name: {module[0]} ] -> Module: {module[1]}\\n')\n",
        "                        name_to_num[module[0]] = num\n",
        "                    else:\n",
        "                        #print(f'[ Number: {num},  Name: {module[0]} ] -> Layer: {module[1]}\\n')\n",
        "                        name_to_num[module[0]] = num\n",
        "\n",
        "                #print('<<      You sholud not select [classifier module], [fc layer] !!      >>')\n",
        "                if sel_module == -1 and next==-1:\n",
        "                    a = 0\n",
        "                elif sel_module==-1 and next==0:\n",
        "                    a =0\n",
        "                elif next>0:\n",
        "                    a=sel_module    \n",
        "                    \n",
        "                  \n",
        "\n",
        "                #print('\\n'*3)\n",
        "                m_val = list(model_obj._modules.values())\n",
        "                m_key = list(model_obj._modules.keys())\n",
        "                if isInt_str(a) == False:\n",
        "                    a = name_to_num[a]\n",
        "                try:\n",
        "                    if any(x in torch.typename(m_val[int(a)]) for x in module_list): \n",
        "                        model_obj = m_val[int(a)]\n",
        "                        name_module = m_key[int(a)]\n",
        "                        if int(a)==0 and next>=0: \n",
        "                          sel_module=18\n",
        "                          next+=1\n",
        "                        elif int(a)==0 and next<0:\n",
        "                          next+=1\n",
        "                        elif int(a)==18:\n",
        "                          sel_module=0  \n",
        "                       \n",
        "                    else:\n",
        "                        self.t_layer = m_val[int(a)]\n",
        "                        run=False\n",
        "                      \n",
        "\n",
        "                except IndexError:\n",
        "                    raise NoIndexError('Selected index (number) is not allowed.')\n",
        "                except KeyError:\n",
        "                    raise NoSuchNameError('Selected name is not allowed.')\n",
        "\n",
        "        else:\n",
        "            model_obj=self.model\n",
        "            # get a target layer from user's input \n",
        "            self.t_layer = choose_tlayer(model_obj)\n",
        "\n",
        "        # hooking for getting feature map\n",
        "        self.t_layer.register_forward_hook(forward_hook)\n",
        "        # hooking for getting gradients\n",
        "        self.t_layer.register_backward_hook(backward_hook)\n",
        "        \n",
        "    def __call__(self):\n",
        "\n",
        "        #print('\\nGradCAM start ... ')\n",
        "\n",
        "        self.img = load_image(self.img_path)\n",
        "\n",
        "        #numpy to tensor and normalize\n",
        "        self.input = preprocess_image(self.img)\n",
        "\n",
        "        output = self.model(self.input)\n",
        "        #for i in range(10):\n",
        "        if self.class_index == None:\n",
        "              # get class index of highest prob among result probabilities\n",
        "            self.class_index = np.argmax(output[0].cpu().data.numpy())\n",
        "            #self.class_index = i\n",
        "\n",
        "        one_hot = np.zeros((1, output[1].size()[-1]), dtype = np.float32)\n",
        "        one_hot[0][self.class_index] = 1\n",
        "        one_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
        "\n",
        "        if cuda_available():\n",
        "            one_hot = torch.sum(one_hot.cuda() * output[1])\n",
        "        else:\n",
        "            one_hot = torch.sum(one_hot * output[1])\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        one_hot.backward(retain_graph = True)\n",
        "          \n",
        "        gradients = self.gradients['value']\n",
        "        activations = self.activations['value']\n",
        "          \n",
        "          #reshaping\n",
        "        weights = torch.mean(torch.mean(gradients, dim = 2), dim=2)\n",
        "        weights = weights.reshape(weights.shape[1], 1, 1)\n",
        "        activationMap = torch.squeeze(activations[0])\n",
        "            \n",
        "          #Get gradcam\n",
        "        gradcam = F.relu((weights*activationMap).sum(0))\n",
        "        gradcam = cv2.resize(gradcam.data.cpu().numpy(), (227,227))\n",
        "        dir_path=save(gradcam, self.img, self.img_path,1)\n",
        "        self.class_index=None\n",
        "        image_resizing_scale = [227,227]\n",
        "        box_scaling = [1,1,1,1] \n",
        "\n",
        "        bbox = BBoxerwGradCAM(model,\n",
        "                      gradcam,\n",
        "                      self.img_path,\n",
        "                      image_resizing_scale,\n",
        "                      box_scaling,\n",
        "                      dir_path,\n",
        "                      1)\n",
        "        bbox.show_smoothheatmap()\n",
        "        bbox.show_bboxrectangle()\n",
        "        bbox.show_bboxpolygon()\n",
        "\n",
        "          #return gradcam\n",
        "        \n",
        "        #print('GradCAM end !!!\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utNWxkQjYAG1"
      },
      "source": [
        "no_cuda = False\n",
        "\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = NIMA().to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Aesthetic/modelsampleattnew29.pt'))\n",
        "grad=GradCAM(path=\"/content/gdrive/MyDrive/Aesthetic/style_image_test/952233.jpg\",model_path=model,select_t_layer=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlpAL5t9Oy2x"
      },
      "source": [
        "from progressbar import ProgressBar\n",
        "pbar=ProgressBar()\n",
        "no_cuda = False\n",
        "\n",
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = NIMA().to(device)\n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/Aesthetic/modelsampleattnew29.pt'))\n",
        "for i in pbar(dataset_test):\n",
        "  img_name='/content/gdrive/MyDrive/Aesthetic/style_image_test/'+str(i['id'])+'.jpg'\n",
        "  grad=GradCAM(path=img_name,model_path=model,select_t_layer=False)\n",
        "  grad()\n",
        "  \n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}